{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8233815,"sourceType":"datasetVersion","datasetId":4883304},{"sourceId":8234920,"sourceType":"datasetVersion","datasetId":4884168}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-20T15:19:47.795972Z","iopub.execute_input":"2024-04-20T15:19:47.796487Z","iopub.status.idle":"2024-04-20T15:19:47.836713Z","shell.execute_reply.started":"2024-04-20T15:19:47.79644Z","shell.execute_reply":"2024-04-20T15:19:47.835734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport lyricsgenius\n\n# Function to check prerequisites and install packages if not already installed\ndef install_packages():\n    try:\n        import lyricsgenius\n    except ImportError:\n        import subprocess\n        subprocess.run(['pip', 'install', 'lyricsgenius'], check=True)\n\n# Setup API\ndef setup_genius():\n    token = '' # Ensure you have set the environment variable appropriately\n    genius = lyricsgenius.Genius(token)\n    genius.remove_section_headers = True\n    genius.timeout = 30\n    genius.skip_non_songs = True\n    genius.excluded_terms = [\"(Remix)\", \"(Live)\", \"(Radio Edit)\"]\n    return genius\n\n# Fetch lyrics for a given artist\ndef fetch_lyrics(genius, artist_name, max_songs=10):\n    artist = genius.search_artist(artist_name, max_songs=max_songs, sort='popularity')\n    return [song.lyrics for song in artist.songs] if artist else []\n\n# Save lyrics to a file\ndef save_lyrics(genius, file_path):\n    genres_artists = {\n        \"Pop\": [\n            \"Michael Jackson\", \"Madonna\", \"Beyoncé\", \"Lady Gaga\", \"Taylor Swift\",\n            \"Justin Bieber\", \"Katy Perry\", \"Britney Spears\", \"Adele\", \"Bruno Mars\"\n        ],\n        \"Rock\": [\n            \"Elvis Presley\", \"Freddie Mercury\", \"David Bowie\", \"Jim Morrison\", \"Mick Jagger\",\n            \"Bruce Springsteen\", \"John Lennon\", \"Paul McCartney\", \"Robert Plant\", \"Kurt Cobain\"\n        ],\n        \"Jazz\": [\n            \"Ella Fitzgerald\", \"Billie Holiday\", \"Louis Armstrong\", \"Nina Simone\", \"Sarah Vaughan\",\n            \"Chet Baker\", \"John Coltrane\", \"Miles Davis\", \"Tony Bennett\", \"Duke Ellington\"\n        ],\n        \"Rap\": [\n            \"Tupac Shakur\", \"Notorious B.I.G.\", \"Jay-Z\", \"Eminem\", \"Kanye West\",\n            \"Nicki Minaj\", \"Snoop Dogg\", \"Kendrick Lamar\", \"Lil Wayne\", \"Drake\"\n        ],\n        \"Country\": [\n            \"Johnny Cash\", \"Dolly Parton\", \"Willie Nelson\", \"Garth Brooks\", \"Shania Twain\",\n            \"Hank Williams\", \"Carrie Underwood\", \"Kenny Rogers\", \"Miranda Lambert\", \"Tim McGraw\"\n        ],\n        \"R&B\": [\n            \"Aretha Franklin\", \"Stevie Wonder\", \"Mariah Carey\", \"Whitney Houston\", \"Usher\",\n            \"Alicia Keys\", \"Ray Charles\", \"Mary J. Blige\", \"R. Kelly\", \"Prince\"\n        ],\n        \"Blues\": [\n            \"B.B. King\", \"Muddy Waters\", \"John Lee Hooker\", \"Ray Charles\", \"Etta James\",\n            \"Robert Johnson\", \"Buddy Guy\", \"Howlin’ Wolf\", \"Janis Joplin\", \"Lead Belly\"\n        ],\n        \"Electronic/Dance\": [\n            \"Daft Punk\", \"Calvin Harris\", \"Avicii\", \"Deadmau5\", \"David Guetta\",\n            \"Skrillex\", \"Tiësto\", \"Diplo\", \"Marshmello\", \"Kygo\"\n        ],\n        \"Reggae\": [\n            \"Bob Marley\", \"Peter Tosh\", \"Bunny Wailer\", \"Gregory Isaacs\", \"Jimmy Cliff\",\n            \"Toots Hibbert\", \"Damian Marley\", \"Sean Paul\", \"Shaggy\", \"Buju Banton\"\n        ]\n    }\n\n    with open(file_path, 'w', encoding='utf-8') as file:\n        for genre, artists in genres_artists.items():\n            for artist in artists:\n                lyrics = fetch_lyrics(genius, artist)\n                for lyric in lyrics:\n                    file.write(f\"[s:genre]{genre}[e:genre][s:lyrics]{lyric}[e:lyrics]\\n\")\n\n# Clean lyrics from the file\ndef clean_lyrics(file_path, output_path):\n    # Regex pattern to find and remove the undesired parts more aggressively\n    contributors_pattern = re.compile(r'\\d+\\s*Contributors.*?Lyrics', re.DOTALL)\n    # Match \"Embed\" followed by any number, ignoring anything up to the next tag or end of line\n    embed_pattern = re.compile(r'\\d+\\s*Embed.*?(?=\\[|$)', re.DOTALL)\n    \n    # Read the input file\n    with open(file_path, 'r', encoding='utf-8') as file:\n        content = file.readlines()\n\n    # Clean each line using the regex pattern\n    cleaned_content = [re.sub(contributors_pattern, '', line) for line in content]\n    cleaned_text = [re.sub(embed_pattern, '', line) for line in cleaned_content]\n\n    # Write the cleaned data to a new file\n    with open(output_path, 'w', encoding='utf-8') as file:\n        file.writelines(cleaned_text)\n\n    print(\"Lyrics cleaned and saved to:\", output_path)\n\ndef clean_lyric_content(lyric_text):\n    # Remove unwanted metadata like contributor lists and translation notes\n    lyric_text = re.sub(r'\\d+\\s*ContributorsTranslations.*?Lyrics', '', lyric_text, flags=re.DOTALL)\n    # Match \"Embed\" followed by any number, ignoring anything up to the next tag or end of line\n    embed_pattern = re.compile(r'\\d+\\s*Embed.*?(?=\\[|$)', re.DOTALL)\n    lyric_text = re.sub(embed_pattern, '', lyric_text)\n    lyric_text = re.sub(r'\\[.*?\\](?!\\[s:|\\[e:)', '', lyric_text)  # Remove all brackets that do not start structural tags\n\n    return lyric_text.strip()\n\n# Main execution flow\nif __name__ == \"__main__\":\n    install_packages()\n    genius_api = setup_genius()\n    lyrics_file_path = ''\n    if not os.path.exists(lyrics_file_path):\n        print('Start using Genius API to load the training data...')\n        save_lyrics(genius_api, lyrics_file_path)\n\n    clean_lyrics_path = ''\n    clean_lyrics(lyrics_file_path, clean_lyrics_path)\n    print(f\"Lyrics cleaned and saved to: {clean_lyrics_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T15:19:47.838874Z","iopub.execute_input":"2024-04-20T15:19:47.839262Z","iopub.status.idle":"2024-04-20T15:25:05.199149Z","shell.execute_reply.started":"2024-04-20T15:19:47.839228Z","shell.execute_reply":"2024-04-20T15:25:05.19797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nif not os.path.exists('/kaggle/working/cleaned_lyrics_data2.txt'):\n    def clean_lyrics(text):\n        # Remove all double quotes\n        text = text.replace('\"', '')\n\n        # Remove or replace special characters if they are not ASCII\n        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n\n        # Replace three or more new lines with exactly two new lines\n        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n\n        return text\n\n    # Example usage\n    with open('/kaggle/working/cleaned_lyrics_data.txt', 'r', encoding='utf-8') as file:\n        data = file.read()\n\n    clean_data = clean_lyrics(data)\n\n    # Write the cleaned data back to a file\n    with open('/kaggle/working/cleaned_lyrics_data2.txt', 'w', encoding='utf-8') as file:\n        file.write(clean_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T15:25:05.201097Z","iopub.execute_input":"2024-04-20T15:25:05.201476Z","iopub.status.idle":"2024-04-20T15:25:05.31002Z","shell.execute_reply.started":"2024-04-20T15:25:05.201443Z","shell.execute_reply":"2024-04-20T15:25:05.309151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nfrom transformers import GPT2TokenizerFast\nimport torch\nimport os\nimport re\nimport numpy as np\nfrom transformers import GPT2LMHeadModel, Trainer, TrainingArguments\nfrom transformers import AdamW, get_scheduler, EarlyStoppingCallback\n\nos.environ['WANDB_DISABLED'] = 'true'\n\nif os.path.exists('/kaggle/input/more-genres/final_cleaned_lyrics.txt'):\n    file_path = '/kaggle/input/more-genres/final_cleaned_lyrics.txt'\nelse:\n    file_path = '/kaggle/working/cleaned_lyrics_data2.txt'\n\nfrom transformers import TrainerCallback\n\nclass PrintLossCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs:\n            print(f\"Step: {state.global_step}, Training Loss: {logs.get('loss', 'N/A')}, Validation Loss: {logs.get('eval_loss', 'N/A')}\")\n\nclass MetricsLoggerCallback(TrainerCallback):\n    def __init__(self):\n        self.train_losses = []\n        self.eval_losses = []\n        self.epochs = []\n\n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        # Save evaluation loss after each evaluation\n        if metrics and 'eval_loss' in metrics:\n            self.eval_losses.append(metrics['eval_loss'])\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        # Save training loss after each logging step\n        if logs and 'loss' in logs:\n            self.train_losses.append(logs['loss'])\n            self.epochs.append(state.epoch)  # Save the current epoch\n\n    def get_metrics(self):\n        return self.epochs, self.train_losses, self.eval_losses\n\nclass LyricsDataset(torch.utils.data.Dataset):\n    def __init__(self, tokenizer, file_path, block_size):\n        with open(file_path, 'r', encoding='utf-8') as f:\n            content = f.read()\n\n        # Split the content into lyric blocks\n        self.lyric_blocks = re.findall(r'\\[s:genre\\].*?\\[e:genre\\]\\[s:lyrics\\](.*?)\\[e:lyrics\\]', content, re.DOTALL)\n        if not self.lyric_blocks:\n            raise ValueError(\"No lyric blocks found. Check the format of the input file.\")\n\n        self.examples = []\n        for block in self.lyric_blocks:\n            bpe_tokens = tokenizer(block.strip(), truncation=True, max_length=block_size, padding=\"max_length\", return_tensors=\"pt\")\n            self.examples.append({\n                'input_ids': bpe_tokens['input_ids'][0],\n                'attention_mask': bpe_tokens['attention_mask'][0],\n                'labels': bpe_tokens['input_ids'][0]  \n            })\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i):\n        return self.examples[i]\n    \n# Load tokenizer\ntokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\nspecial_tokens_dict = {'additional_special_tokens': ['[s:genre]', '[e:genre]', '[s:lyrics]', '[e:lyrics]']}\ntokenizer.add_special_tokens(special_tokens_dict)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Create dataset\nblock_size = 160 # Adjust based on your GPU memory\ndataset = LyricsDataset(tokenizer, file_path, block_size)\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_dataset, eval_dataset = train_test_split(dataset, test_size=0.1)  # 10% for validation\n\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.resize_token_embeddings(len(tokenizer))\n\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\", \n    save_strategy=\"epoch\", \n    logging_strategy=\"epoch\",              \n    num_train_epochs=40,\n    per_device_train_batch_size=36,\n    per_device_eval_batch_size=36,\n    warmup_steps=500,\n    weight_decay=0.001,\n    logging_dir='./logs',\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model='loss',\n    greater_is_better=False\n)\n\noptimizer = AdamW(model.parameters(), lr=5e-6)\n\n# Initialize the learning rate scheduler\nnum_training_steps = len(train_dataset) * training_args.num_train_epochs\nlr_scheduler = get_scheduler(\n    \"cosine\",\n    optimizer=optimizer,\n    num_warmup_steps=500,\n    num_training_steps=num_training_steps,\n)\n\nmetrics_logger = MetricsLoggerCallback()\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    optimizers=(optimizer, lr_scheduler),\n    callbacks=[metrics_logger, EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model and configuration\nmodel.save_pretrained('/kaggle/working/results/best/')\n\ntokenizer.save_pretrained('./results/tokenizer/')\n\nimport json\n\n# Save TrainingArguments to a JSON file\nargs_dict = training_args.to_dict()  # Convert TrainingArguments to a dictionary\nwith open('/kaggle/working/results/training_args.json', 'w') as f:\n    json.dump(args_dict, f, indent=4)\n\n    \nimport matplotlib.pyplot as plt\n\n# Retrieve logged metrics\nepochs, train_losses, eval_losses = metrics_logger.get_metrics()\n\n# Create the plot\nif epochs:\n    plt.figure(figsize=(12, 6))\n    plt.plot(epochs, train_losses, label='Training Loss', marker='o', color='blue')\n    plt.plot(epochs, eval_losses, label='Validation Loss', marker='x', color='orange')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss Over All Epochs')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    if len(epochs) > 20:  # Check if there are at least 20 epochs\n        zoom_range_start = -20  # Last 20 epochs\n        plt.figure(figsize=(12, 6))\n        plt.plot(epochs[zoom_range_start:], train_losses[zoom_range_start:], label='Training Loss', marker='o', color='blue')\n        plt.plot(epochs[zoom_range_start:], eval_losses[zoom_range_start:], label='Validation Loss', marker='x', color='orange')\n        plt.xlabel('Epochs (Last 20)')\n        plt.ylabel('Loss')\n        plt.title('Training and Validation Loss Over Last 20 Epochs')\n        plt.legend()\n        plt.grid(True)\n        plt.show()\n    \nelse:\n    print(\"No data to plot. Check the data collection process.\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T07:34:46.663655Z","iopub.execute_input":"2024-04-26T07:34:46.664070Z","iopub.status.idle":"2024-04-26T07:51:00.177375Z","shell.execute_reply.started":"2024-04-26T07:34:46.664019Z","shell.execute_reply":"2024-04-26T07:51:00.176439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel, GPT2TokenizerFast\nimport torch\nimport re\n\n# Load the model and tokenizer\nmodel = GPT2LMHeadModel.from_pretrained('/kaggle/input/model-parameters/kaggle/working/results/best')\ntokenizer = GPT2TokenizerFast.from_pretrained('/kaggle/input/model-parameters/kaggle/working/results/tokenizer')\n\n# Define the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef generate_lyrics(genre, prompt, min_length=200, max_length=600):\n    genre_context = f\"[s:genre]{genre}[e:genre]\"\n    lyrics_context = f\"[s:lyrics]\"\n    full_prompt = f\"{genre_context}{lyrics_context} {prompt}\"\n    \n    input_ids = tokenizer.encode(full_prompt, return_tensors='pt').to(device)\n\n    eos_token_id = tokenizer.encode('[e:lyrics]', add_special_tokens=False)[0]\n\n    output_ids = model.generate(\n        input_ids,\n        max_length=max_length,\n        min_length=min_length,\n        top_k=50,\n        top_p=0.9,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=eos_token_id,\n        no_repeat_ngram_size=2,\n        do_sample=True,\n        temperature=0.8\n    )\n\n    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n    clean_text = generated_text.split('[e:lyrics]')[0]  # Strip everything after end lyrics tag\n    clean_text = re.sub(r'\\[s:genre\\].*?\\[e:genre\\]', '', clean_text)  # Remove genre tags\n    clean_text = clean_text.replace(lyrics_context, \"\").strip()  # Remove lyrics start tag\n\n    return clean_text\n\ngenres = [\"Pop\", \"Rap\", \"Rock\", \"Jazz\"]\nprompt = False\ninitial_prompt = ''\nfor genre in genres:\n    if prompt:\n        initial_prompt = input(f'Enter the first few lines for {genre} generation, or leave empty for the model to decide:\\n')\n    generated_lyrics = generate_lyrics(genre, initial_prompt)\n    clean_text = re.sub(r'\\[s:genre\\]', '', generated_lyrics) # Remove [s:genre] tags\n    clean_text = re.sub(r'\\<|endoftext|>', '', clean_text)\n    clean_text = re.sub(r'\\[sgenre\\]', '', clean_text) \n    clean_text = re.sub(r'\\[egenre\\]', '', clean_text) \n    clean_text = re.sub(r'\\[e:genre\\]', '', clean_text) \n    clean_text = re.sub(r'\\||', '', clean_text)\n    clean_text = re.sub(r':', '', clean_text)\n\n    print(f\"Generated Lyrics for {genre}:\\n\\n\",clean_text + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-26T07:51:16.308674Z","iopub.execute_input":"2024-04-26T07:51:16.309868Z","iopub.status.idle":"2024-04-26T07:51:48.679061Z","shell.execute_reply.started":"2024-04-26T07:51:16.309829Z","shell.execute_reply":"2024-04-26T07:51:48.678072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n!zip -r file.zip /kaggle/working/results/best/ /kaggle/working/results/tokenizer/\nFileLink(r'file.zip')","metadata":{"execution":{"iopub.status.busy":"2024-04-26T07:53:49.854404Z","iopub.execute_input":"2024-04-26T07:53:49.854838Z","iopub.status.idle":"2024-04-26T07:54:16.996011Z","shell.execute_reply.started":"2024-04-26T07:53:49.854805Z","shell.execute_reply":"2024-04-26T07:54:16.994903Z"},"trusted":true},"execution_count":null,"outputs":[]}]}